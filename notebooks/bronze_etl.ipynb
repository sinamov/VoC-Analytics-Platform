{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f51e8b71-5e15-4d4a-b36c-a2841cfe433f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# variables\n",
    "storage_account_name = \"voc43cdmqlsgj6nostorage\"\n",
    "container_name = \"data\"\n",
    "mount_point = \"/mnt/data\"\n",
    "\n",
    "# Securely fetch the secret key from the scope we created\n",
    "try:\n",
    "    secret_key = dbutils.secrets.get(scope=\"voc_project_scope\", key=\"storage-account-key\")\n",
    "except Exception as e:\n",
    "    print(\"Looks like your secret scope or key isn't set up.\")\n",
    "    raise e\n",
    "\n",
    "# Build the configuration string\n",
    "config = f\"fs.azure.account.key.{storage_account_name}.blob.core.windows.net\"\n",
    "\n",
    "# Check if the mount point already exists (makes our code safe to re-run)\n",
    "if not any(mount.mountPoint == mount_point for mount in dbutils.fs.mounts()):\n",
    "    print(f\"Mounting {mount_point}...\")\n",
    "    \n",
    "    # Mount the storage\n",
    "    dbutils.fs.mount(\n",
    "      source = f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net\",\n",
    "      mount_point = mount_point,\n",
    "      extra_configs = {config: secret_key}\n",
    "    )\n",
    "    print(f\"Successfully mounted {mount_point}!\")\n",
    "else:\n",
    "    print(f\"{mount_point} is already mounted.\")\n",
    "\n",
    "# Verify by listing the files\n",
    "print(\"--- Files in mount point ---\")\n",
    "display(dbutils.fs.ls(mount_point))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5ff6c9e-ac94-4451-b42b-c64dfde0c0f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, explode, split, udf, lit, regexp_replace\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "import mailbox\n",
    "import email # We need the 'email' library to handle parsing exceptions\n",
    "\n",
    "# --- Read the Data ---\n",
    "# Instead of reading line-by-line, we read each .mbox file *entirely* into memory.\n",
    "# This gives us (filePath, fileContent) pairs, which is perfect for mbox parsing.\n",
    "rdd = spark.sparkContext.wholeTextFiles(\"/mnt/data/*.mbox\")\n",
    "df_raw_files = rdd.toDF([\"filepath\", \"raw_text\"])\n",
    "\n",
    "# --- Split Files into Individual Emails ---\n",
    "# An .mbox file separates emails with a line starting with \"From \"\n",
    "# We split the raw_text of each file by this delimiter.\n",
    "# This creates an array of raw email strings.\n",
    "df_split_emails = df_raw_files.select(\n",
    "    \"filepath\",\n",
    "    explode(\n",
    "        split(col(\"raw_text\"), \"\\nFrom \") # Note the \\n and the space\n",
    "    ).alias(\"raw_email_text\")\n",
    ")\n",
    "\n",
    "# --- Define the Parsing UDF (User-Defined Function) ---\n",
    "# This is the \"brain\" of our parser. We use Python's 'mailbox' library\n",
    "# to robustly parse the raw email string.\n",
    "def parse_email_text(raw_text):\n",
    "    \"\"\"\n",
    "    Takes a raw email string and parses it into key-value pairs\n",
    "    using Python's native 'mailbox' library.\n",
    "    \"\"\"\n",
    "    # We must add a \"From \" header back, as we split on it.\n",
    "    # The mailbox library requires it.\n",
    "    try:\n",
    "        # Re-add the separator for the parser to work\n",
    "        msg = mailbox.Message(\"From \" + raw_text)\n",
    "        \n",
    "        # We'll use .get() to avoid errors if a field is missing\n",
    "        return (\n",
    "            msg.get('Message-ID', 'N/A'),\n",
    "            msg.get('From', 'N/A'),\n",
    "            msg.get('Subject', 'N/A'),\n",
    "            msg.get('Date', 'N/A'),\n",
    "            msg.get_payload() # This gets the email body\n",
    "        )\n",
    "    except email.errors.MessageParseError:\n",
    "        # Handle malformed emails gracefully\n",
    "        return ('PARSE_ERROR', 'PARSE_ERROR', 'PARSE_ERROR', 'PARSE_ERROR', 'PARSE_ERROR')\n",
    "\n",
    "# Define the \"schema\" or structure that our UDF will return.\n",
    "# This is critical for Spark's performance.\n",
    "email_schema = StructType([\n",
    "    StructField(\"message_id\", StringType(), True),\n",
    "    StructField(\"sender\", StringType(), True),\n",
    "    StructField(\"subject\", StringType(), True),\n",
    "    StructField(\"date_str\", StringType(), True),\n",
    "    StructField(\"body\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Register our Python function as a Spark UDF\n",
    "parse_email_udf = udf(parse_email_text, email_schema)\n",
    "\n",
    "# --- Apply the UDF and Clean the Data ---\n",
    "# Apply our UDF to the 'raw_email_text' column\n",
    "df_parsed = df_split_emails.withColumn(\"parsed_email\", parse_email_udf(col(\"raw_email_text\")))\n",
    "\n",
    "# \"Flatten\" the new 'parsed_email' struct into top-level columns\n",
    "# and perform our aggressive text cleaning from the project plan.\n",
    "df_bronze_clean = df_parsed.select(\n",
    "    \"filepath\",\n",
    "    col(\"parsed_email.message_id\").alias(\"message_id\"),\n",
    "    col(\"parsed_email.sender\").alias(\"sender\"),\n",
    "    col(\"parsed_email.subject\").alias(\"subject\"),\n",
    "    col(\"parsed_email.date_str\").alias(\"date_str\"),\n",
    "    \n",
    "    # Clean the body:\n",
    "    # 1. Remove all \">\" quoted replies\n",
    "    # 2. Remove common email signatures\n",
    "    # 3. Trim extra whitespace\n",
    "    regexp_replace(col(\"parsed_email.body\"), r\"(?m)^\\>.*\", \"\").alias(\"body_no_quotes\"),\n",
    "    col(\"raw_email_text\") # Keep this for debugging, just in case\n",
    ").where(col(\"message_id\") != \"PARSE_ERROR\") # Filter out any emails that failed to parse\n",
    "\n",
    "# Let's clean the signatures *after* the quotes\n",
    "df_bronze_clean = df_bronze_clean.withColumn(\n",
    "    \"body_clean\",\n",
    "    regexp_replace(col(\"body_no_quotes\"), r\"(?s)--\\n.*\", \"\") # Remove text after signature dashes\n",
    ")\n",
    "\n",
    "# --- Save as \"Bronze\" Delta Lake Table ---\n",
    "# This is the final step. We save our clean DataFrame as a permanent,\n",
    "# queryable table in the Databricks \"Delta Lake\".\n",
    "(df_bronze_clean\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\") # Use \"overwrite\" for a one-time job\n",
    "    .saveAsTable(\"voc_bronze_layer\")\n",
    ")\n",
    "\n",
    "print(\"--- Successfully created voc_bronze_layer Delta table! ---\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_etl",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
