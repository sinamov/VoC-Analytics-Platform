{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f51e8b71-5e15-4d4a-b36c-a2841cfe433f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# variables\n",
    "storage_account_name = \"voc43cdmqlsgj6nostorage\"\n",
    "container_name = \"data\"\n",
    "mount_point = \"/mnt/data\"\n",
    "\n",
    "# Securely fetch the secret key from the scope we created\n",
    "try:\n",
    "    secret_key = dbutils.secrets.get(scope=\"voc_project_scope\", key=\"storage-account-key\")\n",
    "except Exception as e:\n",
    "    print(\"Looks like your secret scope or key isn't set up.\")\n",
    "    raise e\n",
    "\n",
    "# Build the configuration string\n",
    "config = f\"fs.azure.account.key.{storage_account_name}.blob.core.windows.net\"\n",
    "\n",
    "# Check if the mount point already exists (makes our code safe to re-run)\n",
    "if not any(mount.mountPoint == mount_point for mount in dbutils.fs.mounts()):\n",
    "    print(f\"Mounting {mount_point}...\")\n",
    "    \n",
    "    # Mount the storage\n",
    "    dbutils.fs.mount(\n",
    "      source = f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net\",\n",
    "      mount_point = mount_point,\n",
    "      extra_configs = {config: secret_key}\n",
    "    )\n",
    "    print(f\"Successfully mounted {mount_point}!\")\n",
    "else:\n",
    "    print(f\"{mount_point} is already mounted.\")\n",
    "\n",
    "# Verify by listing the files\n",
    "print(\"--- Files in mount point ---\")\n",
    "display(dbutils.fs.ls(mount_point))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9e8c7c1-5f72-4f4d-b10f-42577ccf513b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This is the \"nuke and pave\" method.\n",
    "# It guarantees a fresh start for our ETL job.\n",
    "spark.sql(\"DROP TABLE IF EXISTS voc_bronze_layer\")\n",
    "\n",
    "print(\"Old 'voc_bronze_layer' table (if any) has been dropped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5ff6c9e-ac94-4451-b42b-c64dfde0c0f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import all the functions we'll need\n",
    "from pyspark.sql.functions import col, explode, split, udf, lit, regexp_replace, length\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "import mailbox\n",
    "import email # We need the 'email' library\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 1. --- Read the Data ---\n",
    "# -----------------------------------------------------------------\n",
    "# Read each .mbox file *entirely* into memory\n",
    "rdd = spark.sparkContext.wholeTextFiles(\"/mnt/data/*.mbox\")\n",
    "df_raw_files = rdd.toDF([\"filepath\", \"raw_text\"])\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 2. --- Split Files into Individual Emails ---\n",
    "# -----------------------------------------------------------------\n",
    "# This creates the 'df_split_emails' DataFrame\n",
    "df_split_emails = df_raw_files.select(\n",
    "    \"filepath\",\n",
    "    explode(\n",
    "        split(col(\"raw_text\"), \"\\nFrom \") # Note the \\n and the space\n",
    "    ).alias(\"raw_email_text\")\n",
    ").where(length(col(\"raw_email_text\")) > 10) # Filter out empty splits\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 3. --- Define the Robust Parsing UDF ---\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "def get_plain_text_payload(msg):\n",
    "    \"\"\"\n",
    "    Iterates through a potentially multipart email message\n",
    "    and returns the first 'text/plain' part it finds.\n",
    "    \"\"\"\n",
    "    if msg.is_multipart():\n",
    "        # This is a multipart email, walk through its parts\n",
    "        for part in msg.walk():\n",
    "            if part.get_content_type() == 'text/plain':\n",
    "                try:\n",
    "                    # Decode the payload and handle any character set errors\n",
    "                    return part.get_payload(decode=True).decode(part.get_content_charset() or 'utf-8', 'ignore')\n",
    "                except Exception:\n",
    "                    return None # Return None if decoding fails\n",
    "        return None # No 'text/plain' part found\n",
    "    else:\n",
    "        # This is a simple, single-part email\n",
    "        if msg.get_content_type() == 'text/plain':\n",
    "            try:\n",
    "                return msg.get_payload(decode=True).decode(msg.get_content_charset() or 'utf-8', 'ignore')\n",
    "            except Exception:\n",
    "                return None\n",
    "    return None # Not multipart and not plain text\n",
    "\n",
    "def parse_email_text(raw_text):\n",
    "    \"\"\"\n",
    "    Takes a raw email string, parses it, and uses our helper\n",
    "    to extract the clean, plain-text body.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Re-add the separator for the parser to work\n",
    "        msg = mailbox.Message(\"From \" + raw_text)\n",
    "        \n",
    "        # Use our new helper function to get the body\n",
    "        body = get_plain_text_payload(msg)\n",
    "        \n",
    "        # If no plain text body was found, mark it\n",
    "        if body is None:\n",
    "            body = \"N/A_NO_PLAIN_TEXT\"\n",
    "            \n",
    "        return (\n",
    "            msg.get('Message-ID', 'N/A'),\n",
    "            msg.get('From', 'N/A'),\n",
    "            msg.get('Subject', 'N/A'),\n",
    "            msg.get('Date', 'N/A'),\n",
    "            body\n",
    "        )\n",
    "    except Exception:\n",
    "        # Handle malformed emails gracefully\n",
    "        return ('PARSE_ERROR', 'PARSE_ERROR', 'PARSE_ERROR', 'PARSE_ERROR', 'PARSE_ERROR')\n",
    "\n",
    "# Define the \"schema\" or structure that our UDF will return\n",
    "email_schema = StructType([\n",
    "    StructField(\"message_id\", StringType(), True),\n",
    "    StructField(\"sender\", StringType(), True),\n",
    "    StructField(\"subject\", StringType(), True),\n",
    "    StructField(\"date_str\", StringType(), True),\n",
    "    StructField(\"body\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Register our Python function as a Spark UDF\n",
    "parse_email_udf = udf(parse_email_text, email_schema)\n",
    "\n",
    "print(\"UDF is defined and registered.\")\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 4. --- Apply UDF, Clean, and Save ---\n",
    "# -----------------------------------------------------------------\n",
    "print(\"Applying UDF...\")\n",
    "# Now this line will work, because 'df_split_emails' exists\n",
    "df_parsed = df_split_emails.withColumn(\"parsed_email\", parse_email_udf(col(\"raw_email_text\")))\n",
    "\n",
    "print(\"Cleaning and filtering...\")\n",
    "df_bronze_clean = df_parsed.select(\n",
    "    \"filepath\",\n",
    "    col(\"parsed_email.message_id\").alias(\"message_id\"),\n",
    "    col(\"parsed_email.sender\").alias(\"sender\"),\n",
    "    col(\"parsed_email.subject\").alias(\"subject\"),\n",
    "    col(\"parsed_email.date_str\").alias(\"date_str\"),\n",
    "    col(\"parsed_email.body\").alias(\"body_raw\"),\n",
    "    col(\"raw_email_text\") \n",
    ").where(col(\"message_id\") != \"PARSE_ERROR\") \\\n",
    " .where(col(\"body_raw\") != \"N/A_NO_PLAIN_TEXT\") # Filter out HTML-only emails\n",
    "\n",
    "# Clean the body (now on the 'body_raw' column)\n",
    "df_bronze_clean = df_bronze_clean.withColumn(\n",
    "    \"body_no_quotes\",\n",
    "    regexp_replace(col(\"body_raw\"), r\"(?m)^\\>.*\", \"\") # 1. Remove \">\" quotes\n",
    ").withColumn(\n",
    "    \"body_clean\",\n",
    "    regexp_replace(col(\"body_no_quotes\"), r\"(?s)--\\n.*\", \"\") # 2. Remove signatures\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 5. --- Save as \"Bronze\" Delta Lake Table ---\n",
    "# -----------------------------------------------------------------\n",
    "print(\"Saving to Delta table 'voc_bronze_layer'...\")\n",
    "(df_bronze_clean\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\") # Overwrite the old, messy data\n",
    "    .saveAsTable(\"voc_bronze_layer\")\n",
    ")\n",
    "\n",
    "print(\"--- Successfully REBUILT voc_bronze_layer Delta table! ---\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_etl",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
