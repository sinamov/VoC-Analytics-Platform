{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f51e8b71-5e15-4d4a-b36c-a2841cfe433f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# variables\n",
    "storage_account_name = \"voc43cdmqlsgj6nostorage\"\n",
    "container_name = \"data\"\n",
    "mount_point = \"/mnt/data\"\n",
    "\n",
    "# Securely fetch the secret key from the scope we created\n",
    "try:\n",
    "    secret_key = dbutils.secrets.get(scope=\"voc_project_scope\", key=\"storage-account-key\")\n",
    "except Exception as e:\n",
    "    print(\"Looks like your secret scope or key isn't set up.\")\n",
    "    raise e\n",
    "\n",
    "# Build the configuration string\n",
    "config = f\"fs.azure.account.key.{storage_account_name}.blob.core.windows.net\"\n",
    "\n",
    "# Check if the mount point already exists (makes our code safe to re-run)\n",
    "if not any(mount.mountPoint == mount_point for mount in dbutils.fs.mounts()):\n",
    "    print(f\"Mounting {mount_point}...\")\n",
    "    \n",
    "    # Mount the storage\n",
    "    dbutils.fs.mount(\n",
    "      source = f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net\",\n",
    "      mount_point = mount_point,\n",
    "      extra_configs = {config: secret_key}\n",
    "    )\n",
    "    print(f\"Successfully mounted {mount_point}!\")\n",
    "else:\n",
    "    print(f\"{mount_point} is already mounted.\")\n",
    "\n",
    "# Verify by listing the files\n",
    "print(\"--- Files in mount point ---\")\n",
    "display(dbutils.fs.ls(mount_point))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5ff6c9e-ac94-4451-b42b-c64dfde0c0f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import all the functions we'll need\n",
    "from pyspark.sql.functions import col, explode, split, udf, lit, regexp_replace, length, trim\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "import mailbox\n",
    "import email\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 1. --- Drop the Old Table for a Fresh Start ---\n",
    "# -----------------------------------------------------------------\n",
    "# This makes our script \"idempotent\" (re-runnable)\n",
    "print(\"Dropping old table (if it exists) to prevent schema errors...\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS voc_bronze_layer\")\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 2. --- Read the Data ---\n",
    "# -----------------------------------------------------------------\n",
    "print(\"Reading raw .mbox files...\")\n",
    "rdd = spark.sparkContext.wholeTextFiles(\"/mnt/data/*.mbox\")\n",
    "df_raw_files = rdd.toDF([\"filepath\", \"raw_text\"])\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 3. --- Split Files into Individual Emails ---\n",
    "# -----------------------------------------------------------------\n",
    "df_split_emails = df_raw_files.select(\n",
    "    \"filepath\",\n",
    "    explode(\n",
    "        split(col(\"raw_text\"), \"\\nFrom \")\n",
    "    ).alias(\"raw_email_text\")\n",
    ").where(length(col(\"raw_email_text\")) > 10)\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 4. --- Define the Robust Parsing UDF ---\n",
    "# -----------------------------------------------------------------\n",
    "def get_plain_text_payload(msg):\n",
    "    if msg.is_multipart():\n",
    "        for part in msg.walk():\n",
    "            if part.get_content_type() == 'text/plain':\n",
    "                try:\n",
    "                    return part.get_payload(decode=True).decode(part.get_content_charset() or 'utf-8', 'ignore')\n",
    "                except Exception:\n",
    "                    return None\n",
    "        return None\n",
    "    else:\n",
    "        if msg.get_content_type() == 'text/plain':\n",
    "            try:\n",
    "                return msg.get_payload(decode=True).decode(msg.get_content_charset() or 'utf-8', 'ignore')\n",
    "            except Exception:\n",
    "                return None\n",
    "    return None\n",
    "\n",
    "def parse_email_text(raw_text):\n",
    "    try:\n",
    "        msg = mailbox.Message(\"From \" + raw_text)\n",
    "        body = get_plain_text_payload(msg)\n",
    "        if body is None:\n",
    "            body = \"N/A_NO_PLAIN_TEXT\"\n",
    "        return (\n",
    "            msg.get('Message-ID', 'N/A'),\n",
    "            msg.get('From', 'N/A'),\n",
    "            msg.get('Subject', 'N/A'),\n",
    "            msg.get('Date', 'N/A'),\n",
    "            body\n",
    "        )\n",
    "    except Exception:\n",
    "        return ('PARSE_ERROR', 'PARSE_ERROR', 'PARSE_ERROR', 'PARSE_ERROR', 'PARSE_ERROR')\n",
    "\n",
    "email_schema = StructType([\n",
    "    StructField(\"message_id\", StringType(), True),\n",
    "    StructField(\"sender\", StringType(), True),\n",
    "    StructField(\"subject\", StringType(), True),\n",
    "    StructField(\"date_str\", StringType(), True),\n",
    "    StructField(\"body\", StringType(), True)\n",
    "])\n",
    "\n",
    "parse_email_udf = udf(parse_email_text, email_schema)\n",
    "print(\"UDF is defined and registered.\")\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 5. --- Apply UDF, Clean, and Save ---\n",
    "# -----------------------------------------------------------------\n",
    "print(\"Applying UDF...\")\n",
    "df_parsed = df_split_emails.withColumn(\"parsed_email\", parse_email_udf(col(\"raw_email_text\")))\n",
    "\n",
    "print(\"Cleaning and filtering...\")\n",
    "df_bronze_clean = df_parsed.select(\n",
    "    \"filepath\",\n",
    "    col(\"parsed_email.message_id\").alias(\"message_id\"),\n",
    "    col(\"parsed_email.sender\").alias(\"sender\"),\n",
    "    col(\"parsed_email.subject\").alias(\"subject\"),\n",
    "    col(\"parsed_email.date_str\").alias(\"date_str\"),\n",
    "    col(\"parsed_email.body\").alias(\"body_raw\"),\n",
    ").where(col(\"message_id\") != \"PARSE_ERROR\") \\\n",
    " .where(col(\"body_raw\") != \"N/A_NO_PLAIN_TEXT\")\n",
    "\n",
    "# --- THIS IS THE NEW CLEANING LOGIC ---\n",
    "print(\"Applying final text cleaning (quotes, signatures, newlines)...\")\n",
    "df_final_clean = df_bronze_clean.withColumn(\n",
    "    \"body_no_quotes\",\n",
    "    regexp_replace(col(\"body_raw\"), r\"(?m)^\\>.*\", \"\") # 1. Remove \">\" quotes\n",
    ").withColumn(\n",
    "    \"body_no_sigs\",\n",
    "    regexp_replace(col(\"body_no_quotes\"), r\"(?s)--\\n.*\", \"\") # 2. Remove signatures\n",
    ").withColumn(\n",
    "    \"body_squashed\",\n",
    "    regexp_replace(col(\"body_no_sigs\"), r\"\\n{3,}\", \"\\n\\n\") # 3. Squash 3+ newlines to 2\n",
    ").withColumn(\n",
    "    \"body_clean\",\n",
    "    trim(col(\"body_squashed\")) # 4. Trim all leading/trailing whitespace\n",
    ")\n",
    "\n",
    "# Select only the columns we want to save\n",
    "df_to_save = df_final_clean.select(\n",
    "    \"message_id\",\n",
    "    \"sender\",\n",
    "    \"subject\",\n",
    "    \"date_str\",\n",
    "    \"body_clean\" # Save the final clean body\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 6. --- Save as \"Bronze\" Delta Lake Table ---\n",
    "# -----------------------------------------------------------------\n",
    "print(\"Saving to Delta table 'voc_bronze_layer'...\")\n",
    "(df_to_save\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\") # This will work now\n",
    "    .saveAsTable(\"voc_bronze_layer\")\n",
    ")\n",
    "\n",
    "print(\"--- Successfully REBUILT voc_bronze_layer Delta table! ---\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_etl",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
