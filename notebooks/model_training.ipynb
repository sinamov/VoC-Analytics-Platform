{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4fa14fa-f252-4820-abf7-c5aa34c2ba4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, explode, split, udf, lit, regexp_replace, length\n",
    "# Load the \"Bronze\" table we created in the spark job in bronze_etl\n",
    "# This is \"reading from the Delta Lake\"\n",
    "df_bronze = spark.table(\"voc_bronze_layer\")\n",
    "\n",
    "# Select just the columns we need for training\n",
    "# We only care about the clean body text\n",
    "df_body_text = df_bronze.select(\"message_id\", \"body_clean\") \\\n",
    "                        .where(col(\"body_clean\").isNotNull()) \\\n",
    "                        .where(length(col(\"body_clean\")) > 100) # Filter out tiny/empty emails\n",
    "\n",
    "# Take a 1000-row sample\n",
    "# This is more than enough for our initial training\n",
    "# We use .limit() for a quick, non-random sample. It's fine for this.\n",
    "df_sample = df_body_text.limit(1000)\n",
    "\n",
    "# Convert to Pandas!\n",
    "# Spark is for \"big data,\" but spaCy and other libraries\n",
    "# are easier to use with a \"Pandas\" DataFrame.\n",
    "# .toPandas() collects all the data from the Spark cluster \n",
    "# onto the driver node as a single, in-memory object.\n",
    "pdf_sample = df_sample.toPandas()\n",
    "\n",
    "# Display the result\n",
    "print(f\"Loaded {len(pdf_sample)} rows into a Pandas DataFrame.\")\n",
    "pdf_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "066bf114-cad3-4978-af16-233004ca0deb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pdf_sample.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bd2c57a-ec3a-4057-b388-d49031ae11de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(pdf_sample['body_clean'].iloc[4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "685174ba-63f0-4195-b750-4ff4f7410744",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let's look at a few clean emails from our sample to get started\n",
    "# We'll print the text AND its index in the pandas DataFrame\n",
    "# so we can easily copy-paste it.\n",
    "\n",
    "print(\"--- Email at index 0 ---\")\n",
    "print(pdf_sample['body_clean'].iloc[0])\n",
    "print(\"\\n--- Email at index 1 ---\")\n",
    "print(pdf_sample['body_clean'].iloc[1])\n",
    "print(\"\\n--- Email at index 2 ---\")\n",
    "print(pdf_sample['body_clean'].iloc[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98731b9f-5d1b-499f-b79c-bc44e840ee67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Our list of \"seed\" keywords to bootstrap the labeling\n",
    "ASPECT_KEYWORDS = [\n",
    "    # Core Components\n",
    "    \"Spark Core\", \"scheduler\", \"task scheduling\", \"dynamic allocation\", \"shuffle\",\n",
    "    \n",
    "    # Spark SQL & DataFrames\n",
    "    \"Spark SQL\", \"DataFrame\", \"Dataset API\", \"query optimizer\", \"Catalyst\",\n",
    "    \"AQE (Adaptive Query Execution)\", \"query execution\", \"data source\",\n",
    "    \n",
    "    # Streaming\n",
    "    \"Structured Streaming\", \"micro-batch\", \"streaming query\", \"DStream\",\n",
    "    \"streaming performance\", \"latency\", \"watermarking\", \"stateful streaming\",\n",
    "    \n",
    "    # Performance & ML\n",
    "    \"performance tuning\", \"MLlib\", \"caching\", \"memory management\", \"data skew\",\n",
    "    \n",
    "    # Other\n",
    "    \"Spark Connect\", \"PySpark\", \"data source API\", \"connector\"\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(ASPECT_KEYWORDS)} aspect keywords.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d9885cf-a30e-482f-b740-58d583f02662",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# This will hold our \"draft\" of the training data\n",
    "TRAINING_DATA_DRAFT = []\n",
    "\n",
    "# Loop through every email in our Pandas sample\n",
    "for index, row in pdf_sample.iterrows():\n",
    "    text = row['body_clean']\n",
    "    \n",
    "    # This will hold the (start, end, \"ASPECT\") tuples for this one email\n",
    "    entities = []\n",
    "    \n",
    "    # Loop through our keyword list\n",
    "    for keyword in ASPECT_KEYWORDS:\n",
    "        \n",
    "        # Use 're.finditer' to find ALL matches of the keyword, ignoring case\n",
    "        # 're.IGNORECASE' makes it find \"spark sql\" and \"Spark SQL\"\n",
    "        # 're.escape' handles keywords with special chars like \"Spark Core\"\n",
    "        try:\n",
    "            for match in re.finditer(re.escape(keyword), text, re.IGNORECASE):\n",
    "                start, end = match.span()\n",
    "                entities.append( (start, end, \"ASPECT\") )\n",
    "        except re.error as e:\n",
    "            # This handles any regex errors, though it's rare with re.escape\n",
    "            print(f\"Regex error with keyword '{keyword}': {e}\")\n",
    "            \n",
    "    # CRITICAL: We only add the email if we found at least one entity.\n",
    "    # This creates a \"biased\" dataset, which we must be aware of.\n",
    "    if entities:\n",
    "        # Check for overlapping entities (a common problem)\n",
    "        # For this simple script, we'll just take all of them.\n",
    "        # A more advanced script would merge overlaps.\n",
    "        \n",
    "        # Add the full text and the found entities to our draft\n",
    "        TRAINING_DATA_DRAFT.append( (text, {\"entities\": entities}) )\n",
    "\n",
    "print(f\"--- Pre-Labeling Complete ---\")\n",
    "print(f\"Found {len(TRAINING_DATA_DRAFT)} emails with at least one aspect.\")\n",
    "print(\"Here's a sample of what we found:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e513b45-f5e8-472e-8a01-8aac09c36a28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Helper to review our draft\n",
    "# Change 'i' to look at different examples (0, 1, 2, ... 152)\n",
    "\n",
    "i = 100  # <-- CHANGE THIS NUMBER TO SEE THE NEXT EXAMPLE\n",
    "\n",
    "(text, data) = TRAINING_DATA_DRAFT[i]\n",
    "\n",
    "print(f\"--- Reviewing Example {i} ---\")\n",
    "print(\"\\nFULL TEXT:\\n\")\n",
    "print(text)\n",
    "print(\"\\n------------------------------\")\n",
    "print(f\"\\nFOUND LABELS: {data['entities']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec9e7373-a8ec-46cc-bf54-284813c687a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This is our FINAL, clean training data, based on our review.\n",
    "\n",
    "# After reviewing, I found that some examples were good. I limited myself to 25 of em becasue of the tediousness of reveiewing them.\n",
    "# I will copy them here.\n",
    "\n",
    "TRAINING_DATA = [\n",
    "    TRAINING_DATA_DRAFT[0],  # This one looked good\n",
    "    TRAINING_DATA_DRAFT[1],  # This one also looked good\n",
    "    TRAINING_DATA_DRAFT[10],  # This one was good too\n",
    "    TRAINING_DATA_DRAFT[14],  \n",
    "    TRAINING_DATA_DRAFT[21],\n",
    "    TRAINING_DATA_DRAFT[23],\n",
    "    TRAINING_DATA_DRAFT[25],\n",
    "    TRAINING_DATA_DRAFT[26],\n",
    "    TRAINING_DATA_DRAFT[29],\n",
    "    TRAINING_DATA_DRAFT[35],\n",
    "    TRAINING_DATA_DRAFT[39],\n",
    "    TRAINING_DATA_DRAFT[42],\n",
    "    TRAINING_DATA_DRAFT[44],\n",
    "    TRAINING_DATA_DRAFT[45],\n",
    "    TRAINING_DATA_DRAFT[46], \n",
    "    TRAINING_DATA_DRAFT[49],\n",
    "    TRAINING_DATA_DRAFT[51],\n",
    "    TRAINING_DATA_DRAFT[52],\n",
    "    TRAINING_DATA_DRAFT[58],\n",
    "    TRAINING_DATA_DRAFT[65],\n",
    "    TRAINING_DATA_DRAFT[70],\n",
    "    TRAINING_DATA_DRAFT[78],\n",
    "    TRAINING_DATA_DRAFT[79],\n",
    "    TRAINING_DATA_DRAFT[92],\n",
    "    TRAINING_DATA_DRAFT[100],\n",
    "]\n",
    "\n",
    "print(f\"Final, reviewed TRAINING_DATA contains {len(TRAINING_DATA)} examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3363842-8480-4bad-9585-d6e1084e493a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "model_training",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
