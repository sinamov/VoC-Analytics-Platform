{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4fa14fa-f252-4820-abf7-c5aa34c2ba4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, explode, split, udf, lit, regexp_replace, length\n",
    "# Load the \"Bronze\" table we created in the spark job in bronze_etl\n",
    "# This is \"reading from the Delta Lake\"\n",
    "df_bronze = spark.table(\"voc_bronze_layer\")\n",
    "\n",
    "# Select just the columns we need for training\n",
    "# We only care about the clean body text\n",
    "df_body_text = df_bronze.select(\"message_id\", \"body_clean\") \\\n",
    "                        .where(col(\"body_clean\").isNotNull()) \\\n",
    "                        .where(length(col(\"body_clean\")) > 100) # Filter out tiny/empty emails\n",
    "\n",
    "# Take a 1000-row sample\n",
    "# This is more than enough for our initial training\n",
    "# We use .limit() for a quick, non-random sample. It's fine for this.\n",
    "df_sample = df_body_text.limit(1000)\n",
    "\n",
    "# Convert to Pandas!\n",
    "# Spark is for \"big data,\" but spaCy and other libraries\n",
    "# are easier to use with a \"Pandas\" DataFrame.\n",
    "# .toPandas() collects all the data from the Spark cluster \n",
    "# onto the driver node as a single, in-memory object.\n",
    "pdf_sample = df_sample.toPandas()\n",
    "\n",
    "# Display the result\n",
    "print(f\"Loaded {len(pdf_sample)} rows into a Pandas DataFrame.\")\n",
    "pdf_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "066bf114-cad3-4978-af16-233004ca0deb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pdf_sample.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bd2c57a-ec3a-4057-b388-d49031ae11de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(pdf_sample['body_clean'].iloc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d88f024-5170-4cc3-95e8-16c1980ed6f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "model_training",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
