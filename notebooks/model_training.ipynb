{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cacc9135-6b88-4591-9400-6ad8855d2d04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install torch\n",
    "%pip install transformers\n",
    "%pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0056c95-7576-4423-af81-cdc8c2a26698",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a68d232-44f7-417a-95cf-94191eb960ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, explode, split, udf, lit, regexp_replace, length\n",
    "import re\n",
    "import spacy\n",
    "import random\n",
    "import os\n",
    "from spacy.training import Example\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4fa14fa-f252-4820-abf7-c5aa34c2ba4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the \"Bronze\" table we created in the spark job in bronze_etl\n",
    "# This is \"reading from the Delta Lake\"\n",
    "df_bronze = spark.table(\"voc_bronze_layer\")\n",
    "\n",
    "# Select just the columns we need for training\n",
    "# We only care about the clean body text\n",
    "df_body_text = df_bronze.select(\"message_id\", \"body_clean\") \\\n",
    "                        .where(col(\"body_clean\").isNotNull()) \\\n",
    "                        .where(length(col(\"body_clean\")) > 100) # Filter out tiny/empty emails\n",
    "\n",
    "# Take a 1000-row sample\n",
    "# This is more than enough for our initial training\n",
    "# We use .limit() for a quick, non-random sample. It's fine for this.\n",
    "df_sample = df_body_text.limit(1000)\n",
    "\n",
    "# Convert to Pandas!\n",
    "# Spark is for \"big data,\" but spaCy and other libraries\n",
    "# are easier to use with a \"Pandas\" DataFrame.\n",
    "# .toPandas() collects all the data from the Spark cluster \n",
    "# onto the driver node as a single, in-memory object.\n",
    "pdf_sample = df_sample.toPandas()\n",
    "\n",
    "# Display the result\n",
    "print(f\"Loaded {len(pdf_sample)} rows into a Pandas DataFrame.\")\n",
    "pdf_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "066bf114-cad3-4978-af16-233004ca0deb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pdf_sample.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bd2c57a-ec3a-4057-b388-d49031ae11de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(pdf_sample['body_clean'].iloc[4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98731b9f-5d1b-499f-b79c-bc44e840ee67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Our list of \"seed\" keywords to bootstrap the labeling\n",
    "ASPECT_KEYWORDS = [\n",
    "    # Core Components\n",
    "    \"Spark Core\", \"scheduler\", \"task scheduling\", \"dynamic allocation\", \"shuffle\",\n",
    "    \n",
    "    # Spark SQL & DataFrames\n",
    "    \"Spark SQL\", \"DataFrame\", \"Dataset API\", \"query optimizer\", \"Catalyst\",\n",
    "    \"AQE (Adaptive Query Execution)\", \"query execution\", \"data source\",\n",
    "    \n",
    "    # Streaming\n",
    "    \"Structured Streaming\", \"micro-batch\", \"streaming query\", \"DStream\",\n",
    "    \"streaming performance\", \"latency\", \"watermarking\", \"stateful streaming\",\n",
    "    \n",
    "    # Performance & ML\n",
    "    \"performance tuning\", \"MLlib\", \"caching\", \"memory management\", \"data skew\",\n",
    "    \n",
    "    # Other\n",
    "    \"Spark Connect\", \"PySpark\", \"data source API\", \"connector\"\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(ASPECT_KEYWORDS)} aspect keywords.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d9885cf-a30e-482f-b740-58d583f02662",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This will hold our \"draft\" of the training data\n",
    "TRAINING_DATA_DRAFT = []\n",
    "\n",
    "# Loop through every email in our Pandas sample\n",
    "for index, row in pdf_sample.iterrows():\n",
    "    text = row['body_clean']\n",
    "    \n",
    "    # This will hold the (start, end, \"ASPECT\") tuples for this one email\n",
    "    entities = []\n",
    "    \n",
    "    # Loop through our keyword list\n",
    "    for keyword in ASPECT_KEYWORDS:\n",
    "        \n",
    "        # Use 're.finditer' to find ALL matches of the keyword, ignoring case\n",
    "        # 're.IGNORECASE' makes it find \"spark sql\" and \"Spark SQL\"\n",
    "        # 're.escape' handles keywords with special chars like \"Spark Core\"\n",
    "        try:\n",
    "            for match in re.finditer(re.escape(keyword), text, re.IGNORECASE):\n",
    "                start, end = match.span()\n",
    "                entities.append( (start, end, \"ASPECT\") )\n",
    "        except re.error as e:\n",
    "            # This handles any regex errors, though it's rare with re.escape\n",
    "            print(f\"Regex error with keyword '{keyword}': {e}\")\n",
    "            \n",
    "    # CRITICAL: We only add the email if we found at least one entity.\n",
    "    # This creates a \"biased\" dataset, which we must be aware of.\n",
    "    if entities:\n",
    "        # Check for overlapping entities (a common problem)\n",
    "        # For this simple script, we'll just take all of them.\n",
    "        # A more advanced script would merge overlaps.\n",
    "        \n",
    "        # Add the full text and the found entities to our draft\n",
    "        TRAINING_DATA_DRAFT.append( (text, {\"entities\": entities}) )\n",
    "\n",
    "print(f\"--- Pre-Labeling Complete ---\")\n",
    "print(f\"Found {len(TRAINING_DATA_DRAFT)} emails with at least one aspect.\")\n",
    "print(\"Here's a sample of what we found:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e513b45-f5e8-472e-8a01-8aac09c36a28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Helper to review our draft\n",
    "# Change 'i' to look at different examples (0, 1, 2, ... 152)\n",
    "\n",
    "i = 100  # <-- CHANGE THIS NUMBER TO SEE THE NEXT EXAMPLE\n",
    "\n",
    "(text, data) = TRAINING_DATA_DRAFT[i]\n",
    "\n",
    "print(f\"--- Reviewing Example {i} ---\")\n",
    "print(\"\\nFULL TEXT:\\n\")\n",
    "print(text)\n",
    "print(\"\\n------------------------------\")\n",
    "print(f\"\\nFOUND LABELS: {data['entities']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec9e7373-a8ec-46cc-bf54-284813c687a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This is our FINAL, clean training data, based on our review.\n",
    "\n",
    "# After reviewing, I found that some examples were good. I limited myself to 25 of em becasue of the tediousness of reveiewing them.\n",
    "# I will copy them here.\n",
    "\n",
    "TRAINING_DATA = [\n",
    "    TRAINING_DATA_DRAFT[0],  # This one looked good\n",
    "    TRAINING_DATA_DRAFT[1],  # This one also looked good\n",
    "    TRAINING_DATA_DRAFT[10],  # This one was good too\n",
    "    TRAINING_DATA_DRAFT[14],  \n",
    "    TRAINING_DATA_DRAFT[21],\n",
    "    TRAINING_DATA_DRAFT[23],\n",
    "    TRAINING_DATA_DRAFT[25],\n",
    "    TRAINING_DATA_DRAFT[26],\n",
    "    TRAINING_DATA_DRAFT[29],\n",
    "    TRAINING_DATA_DRAFT[35],\n",
    "    TRAINING_DATA_DRAFT[39],\n",
    "    TRAINING_DATA_DRAFT[42],\n",
    "    TRAINING_DATA_DRAFT[44],\n",
    "    TRAINING_DATA_DRAFT[45],\n",
    "    TRAINING_DATA_DRAFT[46], \n",
    "    TRAINING_DATA_DRAFT[49],\n",
    "    TRAINING_DATA_DRAFT[51],\n",
    "    TRAINING_DATA_DRAFT[52],\n",
    "    TRAINING_DATA_DRAFT[58],\n",
    "    TRAINING_DATA_DRAFT[65],\n",
    "    TRAINING_DATA_DRAFT[70],\n",
    "    TRAINING_DATA_DRAFT[78],\n",
    "    TRAINING_DATA_DRAFT[79],\n",
    "    TRAINING_DATA_DRAFT[92],\n",
    "    TRAINING_DATA_DRAFT[100],\n",
    "]\n",
    "\n",
    "print(f\"Final, reviewed TRAINING_DATA contains {len(TRAINING_DATA)} examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3363842-8480-4bad-9585-d6e1084e493a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Create a Blank \"Newborn\" Model ---\n",
    "# We start with a blank English model. It knows nothing.\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# --- Add the \"Aspect\" Label to the Model ---\n",
    "# We add the \"Named Entity Recognition\" (NER) tool to its brain.\n",
    "if \"ner\" not in nlp.pipe_names:\n",
    "    ner = nlp.add_pipe(\"ner\")\n",
    "else:\n",
    "    ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "# We teach the NER tool the *only* label we care about: \"ASPECT\"\n",
    "ner.add_label(\"ASPECT\")\n",
    "\n",
    "# --- Start the Training ---\n",
    "print(\"Starting training...\")\n",
    "n_iter = 20 # We will show the model our textbook 20 times\n",
    "\n",
    "# \"Open the model's brain\" to start learning\n",
    "optimizer = nlp.begin_training()\n",
    "\n",
    "for itn in range(n_iter):\n",
    "    # Shuffle the examples each time so it doesn't just memorize the order\n",
    "    random.shuffle(TRAINING_DATA)\n",
    "    losses = {}\n",
    "    \n",
    "    # --- This is the main training loop ---\n",
    "    for text, annotations in TRAINING_DATA:\n",
    "        try:\n",
    "            # Create an \"Example\" object (the text + the correct answers)\n",
    "            doc = nlp.make_doc(text)\n",
    "            example = Example.from_dict(doc, annotations)\n",
    "            \n",
    "            # --- This is the magic line ---\n",
    "            # 1. Model \"guesses\" on the example\n",
    "            # 2. Compares its guess to the correct answer\n",
    "            # 3. Updates its brain (weights) to get better\n",
    "            nlp.update([example], drop=0.5, losses=losses, sgd=optimizer)\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Handle any bad data, e.g. overlapping entities\n",
    "            # print(f\"Skipping bad data: {e}\")\n",
    "            pass\n",
    "            \n",
    "    # Print the \"loss\" (mistake score). We want this number to go DOWN.\n",
    "    print(f\"Iteration {itn+1}/{n_iter}  |  Loss: {losses.get('ner', 0.0)}\")\n",
    "\n",
    "print(\"--- Training Complete ---\")\n",
    "\n",
    "# --- Save the Final, Trained Model ---\n",
    "# We save the model to the 'models/' folder *inside* our Git repo.\n",
    "# The 'notebooks' folder is at /Workspace/Repos/.../notebooks/\n",
    "# So, '../models/' goes \"up one level\" and \"into models/\"\n",
    "output_dir = \"../models/ner_model\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Save the model's brain (all its files) to that folder\n",
    "nlp.to_disk(output_dir)\n",
    "\n",
    "print(f\"Model saved to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "873552a8-95d9-4d99-83bb-62df5b0ca1f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the model we want to use and where to save it\n",
    "MODEL_NAME = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "OUTPUT_DIR = \"../models/sentiment_model\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "print(f\"Downloading model '{MODEL_NAME}'...\")\n",
    "\n",
    "# Download the tokenizer and model from Hugging Face\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "# The model is the 'brain' for sentiment classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(\"Download complete.\")\n",
    "\n",
    "# Save the tokenizer and model files locally\n",
    "# so that the app can use them offline\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(f\"Model and tokenizer saved to: {OUTPUT_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "model_training",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
