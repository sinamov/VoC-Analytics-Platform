{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be67fdf3-dbd3-42c0-8c98-d3832456dd91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install torch\n",
    "%pip install transformers\n",
    "%pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24411b8f-16c5-43d4-aa95-8c5b195425d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2d2a46c-4953-4c41-85e0-3ec04fae7d83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from pyspark.sql.functions import col, udf, explode, lower\n",
    "from pyspark.sql.types import ArrayType, StructType, StructField, StringType\n",
    "from collections import defaultdict\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cd5b6bb-676f-4335-9227-8f2b56e7f13f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------\n",
    "# 1. LOAD LOCAL MODELS (from DBFS)\n",
    "# -----------------------------------------------------------------\n",
    "# Note: Paths in Databricks Repos are relative to the notebook\n",
    "ner_model_path = \"../models/ner_model\"\n",
    "sentiment_model_path = \"../models/sentiment_model\"\n",
    "sentiment_labels = [\"negative\", \"neutral\", \"positive\"]\n",
    "\n",
    "# Load NER Model\n",
    "nlp_ner = spacy.load(ner_model_path)\n",
    "nlp_ner.add_pipe('sentencizer', before='ner')\n",
    "print(\"✅ Custom NER Model Loaded\")\n",
    "\n",
    "# Load Sentiment Model\n",
    "sentiment_tokenizer = AutoTokenizer.from_pretrained(sentiment_model_path)\n",
    "sentiment_model = AutoModelForSequenceClassification.from_pretrained(sentiment_model_path)\n",
    "print(\"✅ Sentiment Model Loaded\")\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 2. DEFINE THE ABSA LOGIC AS A UDF\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "def run_sentiment_analysis(text: str) -> str:\n",
    "    \"\"\"Runs sentiment analysis on a single piece of text.\"\"\"\n",
    "    try:\n",
    "        inputs = sentiment_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = sentiment_model(**inputs)\n",
    "        probabilities = F.softmax(outputs.logits, dim=-1)\n",
    "        predicted_index = torch.argmax(probabilities, dim=-1).item()\n",
    "        return sentiment_labels[predicted_index]\n",
    "    except Exception:\n",
    "        return \"neutral\"\n",
    "\n",
    "def aggregate_sentiments(sentiments: list) -> str:\n",
    "    \"\"\"Aggregates a list of sentiments into one. Negative > Positive > Neutral.\"\"\"\n",
    "    if \"negative\" in sentiments:\n",
    "        return \"negative\"\n",
    "    if \"positive\" in sentiments:\n",
    "        return \"positive\"\n",
    "    return \"neutral\"\n",
    "\n",
    "def absa_spark_udf(text: str) -> list:\n",
    "    \"\"\"\n",
    "    This is our robust 'absa_analyst' logic, rewritten as a\n",
    "    Spark-compatible UDF that returns a list of serializable dicts.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        doc = nlp_ner(text)\n",
    "        aspect_sentiments = defaultdict(list)\n",
    "\n",
    "        if not doc.ents:\n",
    "            sentiment = run_sentiment_analysis(text)\n",
    "            return [{\"aspect\": \"overall\", \"sentiment\": sentiment}]\n",
    "        \n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == \"ASPECT\":\n",
    "                sentence = ent.sent.text\n",
    "                sentiment = run_sentiment_analysis(sentence)\n",
    "                aspect_sentiments[ent.text.lower()].append(sentiment)\n",
    "\n",
    "        if not aspect_sentiments:\n",
    "            return [{\"aspect\": \"overall\", \"sentiment\": run_sentiment_analysis(text)}]\n",
    "\n",
    "        final_results = []\n",
    "        for aspect, sentiments in aspect_sentiments.items():\n",
    "            final_sentiment = aggregate_sentiments(sentiments)\n",
    "            final_results.append({\"aspect\": aspect, \"sentiment\": final_sentiment})\n",
    "        \n",
    "        return final_results\n",
    "    except Exception as e:\n",
    "        return [{\"aspect\": \"error\", \"sentiment\": str(e)}]\n",
    "\n",
    "# Define the UDF's return schema\n",
    "absa_schema = ArrayType(\n",
    "    StructType([\n",
    "        StructField(\"aspect\", StringType()),\n",
    "        StructField(\"sentiment\", StringType())\n",
    "    ])\n",
    ")\n",
    "\n",
    "# Register the UDF\n",
    "absa_udf = udf(absa_spark_udf, absa_schema)\n",
    "print(\"✅ ABSA UDF Registered\")\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 3. RUN THE JOB AND SAVE THE CSV\n",
    "# -----------------------------------------------------------------\n",
    "# Load our clean bronze table\n",
    "df_bronze = spark.table(\"voc_bronze_layer\")\n",
    "\n",
    "# Run the ABSA UDF over the entire 'body_clean' column\n",
    "# This may take a few minutes\n",
    "print(\"Running ABSA UDF over all bronze data...\")\n",
    "df_with_absa = df_bronze.withColumn(\"absa_results\", absa_udf(col(\"body_clean\")))\n",
    "\n",
    "# Explode the results into a flat table\n",
    "# This is the \"melt\" operation we need for the dashboard\n",
    "df_flat = df_with_absa.select(\n",
    "    col(\"message_id\"),\n",
    "    explode(col(\"absa_results\")).alias(\"absa\")\n",
    ").select(\n",
    "    \"message_id\",\n",
    "    col(\"absa.aspect\").alias(\"aspect\"),\n",
    "    col(\"absa.sentiment\").alias(\"sentiment\")\n",
    ")\n",
    "\n",
    "# Filter out errors and \"overall\" (we only want specific aspects)\n",
    "df_final = df_flat.where(col(\"aspect\") != \"error\").where(col(\"aspect\") != \"overall\")\n",
    "\n",
    "print(\"Analysis complete. Converting to Pandas...\")\n",
    "# Collect the results to the driver and convert to Pandas\n",
    "pdf_final = df_final.toPandas()\n",
    "\n",
    "# Save the data as a CSV in your Git repo folder\n",
    "# This path goes up from 'notebooks/' to the root, then into 'app/data/'\n",
    "output_dir = \"../app/data\"\n",
    "output_path = f\"{output_dir}/dashboard_data.csv\"\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "pdf_final.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"✅ Successfully saved dashboard data to {output_path}\")\n",
    "print(f\"Total rows created: {len(pdf_final)}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "3_Create_Dashboard_Data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
